{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Lateral inhibition\n",
    "### Learning letters after learning digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from SpykeTorch import snn\n",
    "from SpykeTorch import functional as sf\n",
    "from SpykeTorch import visualization as vis\n",
    "from SpykeTorch import utils\n",
    "\n",
    "import struct\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#import import_ipynb\n",
    "#from MozafariMNIST2018_class import MozafariMNIST2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rule\n",
    "\n",
    "class STDP(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_layer, learning_rate, use_stabilizer = True, lower_bound = 0, upper_bound = 1):\n",
    "        super(STDP, self).__init__()\n",
    "        self.conv_layer = conv_layer\n",
    "        if isinstance(learning_rate, list):\n",
    "            self.learning_rate = learning_rate\n",
    "        else:\n",
    "            self.learning_rate = [learning_rate] * conv_layer.out_channels\n",
    "        for i in range(conv_layer.out_channels):\n",
    "            self.learning_rate[i] = (Parameter(torch.tensor([self.learning_rate[i][0]])),\n",
    "                            Parameter(torch.tensor([self.learning_rate[i][1]])))\n",
    "            self.register_parameter('ltp_' + str(i), self.learning_rate[i][0])\n",
    "            self.register_parameter('ltd_' + str(i), self.learning_rate[i][1])\n",
    "            self.learning_rate[i][0].requires_grad_(False)\n",
    "            self.learning_rate[i][1].requires_grad_(False)\n",
    "        self.use_stabilizer = use_stabilizer\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "\n",
    "    def get_pre_post_ordering(self, input_spikes, output_spikes, winners):\n",
    "        # accumulating input and output spikes to get latencies\n",
    "        input_latencies = torch.sum(input_spikes, dim=0)\n",
    "        output_latencies = torch.sum(output_spikes, dim=0)\n",
    "        result = []\n",
    "        for winner in winners:\n",
    "            # generating repeated output tensor with the same size of the receptive field\n",
    "            out_tensor = torch.ones(*self.conv_layer.kernel_size, device=output_latencies.device) * output_latencies[winner]\n",
    "            # slicing input tensor with the same size of the receptive field centered around winner\n",
    "            # since there is no padding, there is no need to shift it to the center\n",
    "            in_tensor = input_latencies[:,winner[-2]:winner[-2]+self.conv_layer.kernel_size[-2],winner[-1]:winner[-1]+self.conv_layer.kernel_size[-1]]\n",
    "            result.append(torch.ge(in_tensor,out_tensor))\n",
    "        return result\n",
    "\n",
    "    # simple STDP rule\n",
    "    # gets prepost pairings, winners, weights, and learning rates (all shoud be tensors)\n",
    "    def forward(self, input_spikes, potentials, output_spikes, winners=None, freeze_tensor=None, kwta = 1, inhibition_radius = 0):\n",
    "        if winners is None:\n",
    "            winners = sf.get_k_winners(potentials, kwta, inhibition_radius, output_spikes)\n",
    "        pairings = self.get_pre_post_ordering(input_spikes, output_spikes, winners)\n",
    "\n",
    "        lr = torch.zeros_like(self.conv_layer.weight)\n",
    "        for i in range(len(winners)):\n",
    "            f = winners[i][0]\n",
    "            lr[f] = torch.where(pairings[i], *(self.learning_rate[f]))\n",
    "\n",
    "        # additional script for weights freezing    \n",
    "        if freeze_tensor is None:\n",
    "            self.conv_layer.weight += lr * ((self.conv_layer.weight-self.lower_bound) * \\\n",
    "                                (self.upper_bound-self.conv_layer.weight) if self.use_stabilizer else 1)\n",
    "        else:\n",
    "            ones_like_freeze_tensor = torch.full_like(freeze_tensor, 1)\n",
    "            zero_like_freeze_tensor = torch.zeros_like(freeze_tensor)\n",
    "\n",
    "            # condition 'freeze_tensor > 0.01' is stronger version of 'freeze_tensor > 0' for insure\n",
    "            anti_freeze_tensor_byte = torch.where(freeze_tensor > 0.01, zero_like_freeze_tensor, ones_like_freeze_tensor)\n",
    "            self.conv_layer.weight += lr * ((self.conv_layer.weight-self.lower_bound) * \\\n",
    "                                (self.upper_bound-self.conv_layer.weight) if self.use_stabilizer else 1) * \\\n",
    "                                anti_freeze_tensor_byte\n",
    "\n",
    "        self.conv_layer.weight.clamp_(self.lower_bound, self.upper_bound)\n",
    "\n",
    "    def update_learning_rate(self, feature, ap, an):\n",
    "        r\"\"\"Updates learning rate for a specific feature map.\n",
    "\n",
    "        Args:\n",
    "            feature (int): The target feature.\n",
    "            ap (float): LTP rate.\n",
    "            an (float): LTD rate.\n",
    "        \"\"\"\n",
    "        self.learning_rate[feature][0][0] = ap\n",
    "        self.learning_rate[feature][1][0] = an\n",
    "\n",
    "    def update_all_learning_rate(self, ap, an):\n",
    "        r\"\"\"Updates learning rates of all the feature maps to a same value.\n",
    "\n",
    "        Args:\n",
    "            ap (float): LTP rate.\n",
    "            an (float): LTD rate.\n",
    "        \"\"\"\n",
    "        for feature in range(self.conv_layer.out_channels):\n",
    "            self.learning_rate[feature][0][0] = ap\n",
    "            self.learning_rate[feature][1][0] = an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class MozafariMNIST2018(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=0.5, dropout_procedure = False):\n",
    "        \n",
    "        super(MozafariMNIST2018, self).__init__()\n",
    "\n",
    "        self.conv1 = snn.Convolution(6, 30, 5, 0.8, 0.05)\n",
    "        self.conv1_t = 15\n",
    "        self.k1 = 5\n",
    "        self.r1 = 3\n",
    "\n",
    "        self.conv2 = snn.Convolution(30, 250, 3, 0.8, 0.05)\n",
    "        self.conv2_t = 10\n",
    "        self.k2 = 8\n",
    "        self.r2 = 1\n",
    "\n",
    "        self.conv3 = snn.Convolution(250, 200, 5, 0.8, 0.05)\n",
    "        self.number_of_features = 200\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.stdp1 = STDP(self.conv1, (0.004, -0.003))                          \n",
    "        self.stdp2 = STDP(self.conv2, (0.004, -0.003))                         \n",
    "        self.stdp3 = STDP(self.conv3, (0.004, -0.003), False, 0.2, 0.8)         \n",
    "        self.anti_stdp3 = STDP(self.conv3, (-0.004, 0.0005), False, 0.2, 0.8)   \n",
    "        self.max_ap = Parameter(torch.Tensor([0.15]))\n",
    "\n",
    "        self.decision_map = []\n",
    "        for i in range(10):\n",
    "            self.decision_map.extend([i]*20)\n",
    "\n",
    "        self.ctx = {\"input_spikes\":None, \"potentials\":None, \\\n",
    "                    \"output_spikes\":None, \"winners\":None, \\\n",
    "                    \"freeze_tensor\":None}                           # freeze_tensor was added to original ver.            \n",
    "        self.spk_cnt1 = 0\n",
    "        self.spk_cnt2 = 0\n",
    "        \n",
    "        self.dropout_procedure = dropout_procedure\n",
    "        \n",
    "    def forward(self, input, max_layer, freeze_tensor=None):        # freeze_tensor was added to original ver.\n",
    "        \n",
    "        input = sf.pad(input.float(), (2,2,2,2), 0)\n",
    "        \n",
    "        if self.training:\n",
    "            pot = self.conv1(input)\n",
    "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
    "            if max_layer == 1:\n",
    "                self.spk_cnt1 += 1\n",
    "                if self.spk_cnt1 >= 500:\n",
    "                    self.spk_cnt1 = 0\n",
    "                    ap = torch.tensor(self.stdp1.learning_rate[0][0].item(), device=self.stdp1.learning_rate[0][0].device) * 2\n",
    "                    ap = torch.min(ap, self.max_ap)\n",
    "                    an = ap * -0.75\n",
    "                    self.stdp1.update_all_learning_rate(ap.item(), an.item())\n",
    "                pot = sf.pointwise_inhibition(pot)\n",
    "                spk = pot.sign()\n",
    "                winners = sf.get_k_winners(pot, self.k1, self.r1, spk)\n",
    "                self.ctx[\"input_spikes\"] = input\n",
    "                self.ctx[\"potentials\"] = pot\n",
    "                self.ctx[\"output_spikes\"] = spk\n",
    "                self.ctx[\"winners\"] = winners\n",
    "                return spk, pot\n",
    "            \n",
    "            spk_in = sf.pad(sf.pooling(spk, 2, 2), (1,1,1,1))\n",
    "            pot = self.conv2(spk_in)\n",
    "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
    "            if max_layer == 2:\n",
    "                self.spk_cnt2 += 1\n",
    "                if self.spk_cnt2 >= 500:\n",
    "                    self.spk_cnt2 = 0\n",
    "                    ap = torch.tensor(self.stdp2.learning_rate[0][0].item(), device=self.stdp2.learning_rate[0][0].device) * 2\n",
    "                    ap = torch.min(ap, self.max_ap)\n",
    "                    an = ap * -0.75\n",
    "                    self.stdp2.update_all_learning_rate(ap.item(), an.item())\n",
    "                pot = sf.pointwise_inhibition(pot)\n",
    "                spk = pot.sign()\n",
    "                winners = sf.get_k_winners(pot, self.k2, self.r2, spk)\n",
    "                self.ctx[\"input_spikes\"] = spk_in\n",
    "                self.ctx[\"potentials\"] = pot\n",
    "                self.ctx[\"output_spikes\"] = spk\n",
    "                self.ctx[\"winners\"] = winners\n",
    "                return spk, pot\n",
    "            \n",
    "            spk_in = sf.pad(sf.pooling(spk, 3, 3), (2,2,2,2))\n",
    "            pot = self.conv3(spk_in)           \n",
    "            \n",
    "            if self.dropout_procedure:\n",
    "                dropout = torch.ones(self.number_of_features) * self.dropout\n",
    "                to_be_dropped = torch.bernoulli(dropout).nonzero()   \n",
    "                sf.feature_inhibition_(pot, to_be_dropped)\n",
    "            \n",
    "            spk = sf.fire(pot)\n",
    "            winners = sf.get_k_winners(pot, 1, 0, spk)\n",
    "            self.ctx[\"input_spikes\"] = spk_in\n",
    "            self.ctx[\"potentials\"] = pot\n",
    "            self.ctx[\"output_spikes\"] = spk\n",
    "            self.ctx[\"winners\"] = winners\n",
    "            self.ctx[\"freeze_tensor\"] = freeze_tensor\n",
    "\n",
    "            output = -1\n",
    "            if len(winners) != 0:\n",
    "                output = self.decision_map[winners[0][0]]\n",
    "            return output\n",
    "        \n",
    "        else:\n",
    "            pot = self.conv1(input)\n",
    "            spk, pot = sf.fire(pot, self.conv1_t, True)\n",
    "            if max_layer == 1:\n",
    "                return spk, pot\n",
    "            \n",
    "            pot = self.conv2(sf.pad(sf.pooling(spk, 2, 2), (1,1,1,1)))\n",
    "            spk, pot = sf.fire(pot, self.conv2_t, True)\n",
    "            if max_layer == 2:\n",
    "                return spk, pot\n",
    "            pot = self.conv3(sf.pad(sf.pooling(spk, 3, 3), (2,2,2,2)))\n",
    "            spk = sf.fire(pot)\n",
    "            winners = sf.get_k_winners(pot, 1, 0, spk)\n",
    "            output = -1\n",
    "            if len(winners) != 0:\n",
    "                output = self.decision_map[winners[0][0]]\n",
    "            return output\n",
    "\n",
    "    def stdp(self, layer_idx):\n",
    "        if layer_idx == 1:\n",
    "            self.stdp1(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"], self.ctx[\"freeze_tensor\"])\n",
    "        if layer_idx == 2:\n",
    "            self.stdp2(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"], self.ctx[\"freeze_tensor\"])\n",
    "\n",
    "    def update_learning_rates(self, stdp_ap, stdp_an, anti_stdp_ap, anti_stdp_an):\n",
    "               \n",
    "        self.stdp3.update_all_learning_rate(stdp_ap, stdp_an)\n",
    "        self.anti_stdp3.update_all_learning_rate(anti_stdp_an, anti_stdp_ap)  \n",
    " \n",
    "    def reward(self):\n",
    "        self.stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"], self.ctx[\"freeze_tensor\"])\n",
    "\n",
    "    def punish(self):\n",
    "        self.anti_stdp3(self.ctx[\"input_spikes\"], self.ctx[\"potentials\"], self.ctx[\"output_spikes\"], self.ctx[\"winners\"], self.ctx[\"freeze_tensor\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "\n",
    "def train_unsupervise(network, data, layer_idx):\n",
    "    network.train()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data_in = data[i]\n",
    "        if use_cuda:\n",
    "            data_in = data_in.cuda()\n",
    "        network(data_in, layer_idx)\n",
    "        network.stdp(layer_idx)\n",
    "\n",
    "def train_rl(network, data, target, freeze_tensor=None):\n",
    "    network.train()\n",
    "    perf = np.array([0,0,0]) # correct, wrong, silence\n",
    "    for i in range(len(data)):\n",
    "        data_in = data[i]\n",
    "        target_in = target[i]\n",
    "        if use_cuda:\n",
    "            data_in = data_in.cuda()\n",
    "            target_in = target_in.cuda()\n",
    "        d = network(data_in, 3, freeze_tensor)    # freeze_tensor was added to original ver.\n",
    "        if d != -1:\n",
    "            if d == target_in:\n",
    "                perf[0]+=1\n",
    "                network.reward() \n",
    "            else:\n",
    "                perf[1]+=1\n",
    "                network.punish()  \n",
    "        else:\n",
    "            perf[2]+=1\n",
    "    return perf/len(data)\n",
    "\n",
    "def test(network, data, target):\n",
    "    network.eval()\n",
    "    perf = np.array([0,0,0]) # correct, wrong, silence\n",
    "    for i in range(len(data)):\n",
    "        data_in = data[i]\n",
    "        target_in = target[i]\n",
    "        if use_cuda:\n",
    "            data_in = data_in.cuda()\n",
    "            target_in = target_in.cuda()\n",
    "        d = network(data_in, 3)\n",
    "        if d != -1:\n",
    "            if d == target_in:\n",
    "                perf[0]+=1\n",
    "            else:\n",
    "                perf[1]+=1\n",
    "        else:\n",
    "            perf[2]+=1\n",
    "    return perf/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image transformation (see dataset)\n",
    "\n",
    "class S1C1Transform:\n",
    "    \n",
    "    def __init__(self, filter, PIL_type=False, timesteps = 15):\n",
    "        self.PIL_type = PIL_type\n",
    "        self.to_pil_image = transforms.ToPILImage()    \n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.filter = filter\n",
    "        self.temporal_transform = utils.Intensity2Latency(timesteps)\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        #if self.cnt % 10000 == 0:\n",
    "        #    print(f'{self.cnt} images')\n",
    "        if self.PIL_type:\n",
    "            image = self.to_pil_image(image)\n",
    "        self.cnt+=1\n",
    "        image = self.to_tensor(image) * 255\n",
    "        image.unsqueeze_(0)\n",
    "        image = self.filter(image)\n",
    "        image = sf.local_normalization(image, 8)\n",
    "        temporal_image = self.temporal_transform(image)\n",
    "        return temporal_image.sign().byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image transformation (see dataset)\n",
    "\n",
    "kernels = [ utils.DoGKernel(3,3/9,6/9),\n",
    "            utils.DoGKernel(3,6/9,3/9),\n",
    "            utils.DoGKernel(7,7/9,14/9),\n",
    "            utils.DoGKernel(7,14/9,7/9),\n",
    "            utils.DoGKernel(13,13/9,26/9),\n",
    "            utils.DoGKernel(13,26/9,13/9)]\n",
    "\n",
    "filter = utils.Filter(kernels, padding = 6, thresholds = 50)\n",
    "\n",
    "s1c1 = S1C1Transform(filter)\n",
    "s1c1_PIL = S1C1Transform(filter, PIL_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_graph(parametr_set):\n",
    "\n",
    "    plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    plt.plot(parametr_set['epoch'], parametr_set['train']*100, color='cyan', label='train')\n",
    "    plt.plot(parametr_set['epoch'], parametr_set['test']*100, color='blue', marker = 'o', label='test')\n",
    "    plt.plot(parametr_set['epoch'], parametr_set['test_previous']*100, linestyle = ':', color='red', label='test of previous images')\n",
    "    plt.xlabel('epochs', loc='right', fontsize=17)\n",
    "    plt.ylabel('accuracy, %',  loc='top', fontsize=17)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train of 3-rd layer\n",
    "\n",
    "def third_layer(file_name_net, file_name_csv, adaptive_int, previous_epochs, epochs, \n",
    "                train_loader, test_loader, test_previous_loader,\n",
    "                model, apr, anr, app, anp, parametr_set, \n",
    "                steps=None, percent=20, value_for_moved_weights=0.8,\n",
    "                it_continues=False, freeze_procedure=False):  \n",
    "    \n",
    "    '''\n",
    "    file_name_net - name of file for saving state_dict of model\n",
    "    file_name_csv - name of file for saving parameters of model in each epoch\n",
    "    adaptive_int - learning rate parameter\n",
    "    previous_epochs - if before model had training in current period\n",
    "    it_continues - is it continue of 3-rd layer training or not (False or True)\n",
    "    percent - percent of moving weights (calculated from the number of high range weights)\n",
    "    '''\n",
    "\n",
    "    adaptive_min=0 \n",
    "\n",
    "    if not it_continues:\n",
    "\n",
    "        previous_epochs = 0\n",
    "        counter = 0\n",
    "\n",
    "        apr_adapt = ((1.0 - 1.0 / 10) * adaptive_int + adaptive_min) * apr\n",
    "        anr_adapt = ((1.0 - 1.0 / 10) * adaptive_int + adaptive_min) * anr\n",
    "        app_adapt = ((1.0 / 10) * adaptive_int + adaptive_min) * app\n",
    "        anp_adapt = ((1.0 / 10) * adaptive_int + adaptive_min) * anp\n",
    "        \n",
    "        best_train = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
    "        best_test = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
    "        best_test_previous = np.array([0.0,0.0,0.0,0.0]) # correct, wrong, silence, epoch\n",
    "\n",
    "    else:\n",
    "      \n",
    "        if len(parametr_set.loc[parametr_set['test'] == parametr_set['test'].max(), 'epoch']) == 1:\n",
    "            optim_index = int(parametr_set.loc[parametr_set['test'] == parametr_set['test'].max(), 'epoch'].item())\n",
    "        else:\n",
    "            optim_index = int(parametr_set.loc[parametr_set['test'] == parametr_set['test'].max(), 'epoch'].tolist()[-1])\n",
    "\n",
    "        if len(parametr_set.loc[parametr_set['train'] == parametr_set['train'].max(), 'epoch']) == 1:\n",
    "            best_train_index = int(parametr_set.loc[parametr_set['train'] == parametr_set['train'].max(), 'epoch'].item())\n",
    "        else:\n",
    "            best_train_index = int(parametr_set.loc[parametr_set['train'] == parametr_set['train'].max(), 'epoch'].tolist()[-1])\n",
    "\n",
    "        if len(parametr_set.loc[parametr_set['test_previous'] == parametr_set['test_previous'].max(), 'epoch']) == 1:\n",
    "            best_test_previous_index = int(parametr_set.loc[parametr_set['test_previous'] == parametr_set['test_previous'].max(), 'epoch'].item())\n",
    "        else:\n",
    "            best_test_previous_index = int(parametr_set.loc[parametr_set['test_previous'] == parametr_set['test_previous'].max(), 'epoch'].tolist()[-1])\n",
    "        \n",
    "        max_index = int(parametr_set.index.max())\n",
    "        counter = (max_index + 1)\n",
    "\n",
    "        param_best_train = parametr_set['train'].iloc[best_train_index]\n",
    "        param_best_test = parametr_set['test'].iloc[optim_index]\n",
    "        param_best_test_previous = parametr_set['test_previous'].iloc[best_test_previous_index]\n",
    "\n",
    "        apr_adapt = parametr_set['apr_adapt'].iloc[optim_index]\n",
    "        anr_adapt = parametr_set['anr_adapt'].iloc[optim_index]\n",
    "        app_adapt = parametr_set['app_adapt'].iloc[optim_index]\n",
    "        anp_adapt = parametr_set['anp_adapt'].iloc[optim_index]\n",
    "        \n",
    "        for i in range(len(model.stdp3.learning_rate)):\n",
    "            model.stdp3.learning_rate[i][0].fill_(parametr_set['stdp3.learning_rate[0]'].iloc[optim_index])\n",
    "            model.stdp3.learning_rate[i][1].fill_(parametr_set['stdp3.learning_rate[1]'].iloc[optim_index])\n",
    "            model.anti_stdp3.learning_rate[0][0].fill_(parametr_set['anti_stdp3.learning_rate[0]'].iloc[optim_index])\n",
    "            model.anti_stdp3.learning_rate[0][0].fill_(parametr_set['anti_stdp3.learning_rate[1]'].iloc[optim_index])\n",
    "\n",
    "        best_train = np.array([param_best_train,1-param_best_train,0.0,best_train_index]) # correct, wrong, silence, epoch\n",
    "        best_test = np.array([param_best_test,1-param_best_test,0.0,optim_index]) # correct, wrong, silence, epoch\n",
    "        best_test_previous = np.array([param_best_test_previous,1-param_best_test_previous,0.0,best_test_previous_index]) # correct, wrong, silence, epoch\n",
    "    \n",
    "    # list of 3-rd layer weights\n",
    "\n",
    "    dim_0, dim_1, dim_2, dim_3 = tuple(model.conv3.weight.size())\n",
    "    total_size = dim_0 * dim_1 * dim_2 * dim_3\n",
    "  \n",
    "    # indexes of weights\n",
    "    indexes_i = []    \n",
    "    indexes_j = []        \n",
    "    indexes_k = []        \n",
    "    indexes_m = []    \n",
    "    \n",
    "    # values of weights\n",
    "    item_values = []  \n",
    "    \n",
    "    for i in range(dim_0):\n",
    "        for j in range(dim_1):\n",
    "            for k in range(dim_2):\n",
    "                for m in range(dim_3):\n",
    "                    indexes_i.append(i)\n",
    "                    indexes_j.append(j)\n",
    "                    indexes_k.append(k)\n",
    "                    indexes_m.append(m)\n",
    "                    item_values.append(model.conv3.weight[i][j][k][m].item())\n",
    "\n",
    "    indexes_dim_0 = pd.Series(indexes_i, name='dim_0') \n",
    "    indexes_dim_1 = pd.Series(indexes_j, name='dim_1')\n",
    "    indexes_dim_2 = pd.Series(indexes_k, name='dim_2')\n",
    "    indexes_dim_3 = pd.Series(indexes_m, name='dim_3')\n",
    "    item_values = pd.Series(item_values, name='value_0')\n",
    "            \n",
    "    conv3_data = pd.concat([item_values, indexes_dim_0, indexes_dim_1, indexes_dim_2, indexes_dim_3], axis=1)\n",
    "    \n",
    "    high_percent = 85 #percent of high range weights\n",
    "    percentile_value = np.percentile(item_values, high_percent)\n",
    "    \n",
    "    conv3_data['low_range_0'] = 0\n",
    "    conv3_data.loc[conv3_data['value_0'] < percentile_value,'low_range_0'] = 1\n",
    "    \n",
    "    # indexes of freeze weights of third layer before training on the next set                   \n",
    "    conv3_data['freeze_weights'] = 0\n",
    "    conv3_data.loc[conv3_data['value_0'] > percentile_value,'freeze_weights'] = 1      \n",
    "    \n",
    "    if freeze_procedure:\n",
    "        print(f\"During training {conv3_data['freeze_weights'].sum()} weights \"\n",
    "              f\"({conv3_data['freeze_weights'].sum()/total_size*100 :.1f}%) will be freezed\")\n",
    "        \n",
    "        weights_threshold = nn.Threshold(percentile_value, 0)\n",
    "        freeze_tensor = weights_threshold(model.conv3.weight)                     \n",
    "        freeze_list = conv3_data.loc[conv3_data['freeze_weights'] == 1, 'value_0'].to_numpy()\n",
    "    else:\n",
    "        freeze_list=[]\n",
    "        freeze_tensor=None\n",
    "\n",
    "    try:\n",
    "        high_range_counter = conv3_data['low_range_0'].value_counts()[0] \n",
    "    except:\n",
    "        high_range_counter = 1\n",
    " \n",
    "    moving_quantity = int((percent/100)*high_range_counter)      #quantity of moving items in each epoch\n",
    "    #quantity of moving items in each epoch is calculated without taking in account freezed weights\n",
    "    \n",
    "    if steps is None:\n",
    "        steps = int(total_size*high_percent/(100*moving_quantity))   #steps of weights moving \n",
    "    elif steps == 0:\n",
    "        print(f'Training will be performed without weight moving.')\n",
    "    else:\n",
    "        print(f'Weight moving will be during {steps} epochs')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        seconds_epoch_0 = time.time() \n",
    "        \n",
    "        print('-'*50)\n",
    "        print(\"Epoch #: \", epoch + previous_epochs)\n",
    "        \n",
    "        perf_train = np.array([0.0,0.0,0.0]) \n",
    "        \n",
    "        for data,targets in train_loader:\n",
    "                \n",
    "            if epoch < steps: \n",
    "                \n",
    "                print(f'Values of high range weights in epoch#{epoch} [{percentile_value :.3f}:0.800] (top {100-high_percent}%)')\n",
    "                low_range_indexes = list(conv3_data.index[(conv3_data['low_range_'+str(epoch)] == 1)&(conv3_data['freeze_weights'] == 0)])\n",
    "                moving_items = random.sample(low_range_indexes, np.minimum(moving_quantity, len(low_range_indexes)))\n",
    "                moving_indexes = conv3_data.loc[conv3_data.index.isin(moving_items)]\n",
    "\n",
    "                print(f'Quantity of moving points in epoch#{epoch + previous_epochs} = {len(moving_indexes.index)} items' \n",
    "                      f' ({len(moving_indexes.index)/(total_size-high_range_counter)*100 :.1f}% of moving points)')\n",
    "\n",
    "                for q in range(len(moving_indexes.index)):\n",
    "                    model.conv3.weight \\\n",
    "                    [moving_indexes['dim_0'].iloc[q]][moving_indexes['dim_1'].iloc[q]][moving_indexes['dim_2'].iloc[q]][moving_indexes['dim_3'].iloc[q]]. \\\n",
    "                    fill_(np.random.normal(loc=value_for_moved_weights, scale=0.05))  \n",
    "\n",
    "                \n",
    "            perf_train_batch = train_rl(model, data, targets, freeze_tensor)        \n",
    "    \n",
    "            if epoch < steps:  \n",
    "            \n",
    "                # new values of weights (after learning)\n",
    "                item_values = []       \n",
    "                for i in range(dim_0):\n",
    "                    for j in range(dim_1):\n",
    "                        for k in range(dim_2):\n",
    "                            for m in range(dim_3):\n",
    "                                item_values.append(model.conv3.weight[i][j][k][m].item())\n",
    "            \n",
    "                item_values = pd.Series(item_values, name='value_'+str(epoch+1))\n",
    "                percentile_value = np.percentile(item_values, high_percent) #new cutting off high range weights\n",
    "                conv3_data = pd.concat([conv3_data, item_values], axis=1)\n",
    "                \n",
    "                conv3_data['low_range_'+str(epoch+1)] = 0\n",
    "                conv3_data.loc[conv3_data['value_'+str(epoch+1)] < percentile_value,'low_range_'+str(epoch+1)] = 1\n",
    "       \n",
    "            #update adaptive learning rates\n",
    "            apr_adapt = apr * (perf_train_batch[1] * adaptive_int + adaptive_min)\n",
    "            anr_adapt = anr * (perf_train_batch[1] * adaptive_int + adaptive_min)\n",
    "            app_adapt = app * (perf_train_batch[0] * adaptive_int + adaptive_min)\n",
    "            anp_adapt = anp * (perf_train_batch[0] * adaptive_int + adaptive_min)\n",
    "            parametr_set.loc[counter, 'epoch'] = epoch + previous_epochs\n",
    "            parametr_set.loc[counter, 'apr_adapt'] = apr_adapt\n",
    "            parametr_set.loc[counter, 'anr_adapt'] = anr_adapt\n",
    "            parametr_set.loc[counter, 'app_adapt'] = app_adapt\n",
    "            parametr_set.loc[counter, 'anp_adapt'] = anp_adapt\n",
    "            parametr_set.loc[counter, 'stdp3.learning_rate[0]'] = model.stdp3.learning_rate[0][0].item()\n",
    "            parametr_set.loc[counter, 'stdp3.learning_rate[1]'] = model.stdp3.learning_rate[0][1].item()\n",
    "            parametr_set.loc[counter, 'anti_stdp3.learning_rate[0]'] = model.anti_stdp3.learning_rate[0][0].item()\n",
    "            parametr_set.loc[counter, 'anti_stdp3.learning_rate[1]'] = model.anti_stdp3.learning_rate[0][1].item()\n",
    "            parametr_set.loc[counter, 'train'] = perf_train_batch[0]\n",
    "\n",
    "            model.update_learning_rates(apr_adapt, anr_adapt, app_adapt, anp_adapt)\n",
    "            perf_train += perf_train_batch\n",
    "            \n",
    "        perf_train /= len(train_loader)\n",
    "\n",
    "        if best_train[0] <= perf_train[0]:\n",
    "            best_train = np.append(perf_train, epoch + previous_epochs)\n",
    "        print(f\"Current Train: {perf_train[0]*100 :.2f}%\")\n",
    "\n",
    "        for data,targets in test_loader:\n",
    "            perf_test = test(model, data, targets)\n",
    "            parametr_set.loc[counter, 'test'] = perf_test[0]\n",
    "            if best_test[0] <= perf_test[0]:\n",
    "                best_test = np.append(perf_test, epoch + previous_epochs)\n",
    "                torch.save(model.state_dict(), file_name_net)\n",
    "            print(f\"Current Test: {perf_test[0]*100 :.2f}%\")\n",
    "\n",
    "        if isinstance(test_previous_loader, DataLoader):\n",
    "            for data,targets in test_previous_loader:\n",
    "                perf_test_previous = test(model, data, targets)\n",
    "                parametr_set.loc[counter, 'test_previous'] = perf_test_previous[0]\n",
    "                if best_test_previous[0] <= perf_test_previous[0]:\n",
    "                    best_test_previous = np.append(perf_test_previous, epoch + previous_epochs)\n",
    "                print(f\"Current Test Previous: {perf_test_previous[0]*100 :.2f}%\")\n",
    "                \n",
    "        else:\n",
    "            parametr_set.loc[counter, 'test_previous'] = 0\n",
    "            \n",
    "        counter += 1\n",
    "                                                 \n",
    "        seconds_epoch_1 = time.time()  \n",
    "        print(f'Operational time of epoch #{epoch + previous_epochs}: '\n",
    "                  f'{int((seconds_epoch_1 - seconds_epoch_0)//60)} min {int((seconds_epoch_1 - seconds_epoch_0)%60)} sec') \n",
    "    \n",
    "    parametr_set.to_csv(file_name_csv)\n",
    "    \n",
    "    print('=='*10, 'SUMMARY', '=='*10)\n",
    "    print(f\"        Best Train: {best_train[0]*100 :.2f}% on {best_train[3] :.0f} epoch\")\n",
    "    print(f\"         Best Test: {best_test[0]*100 :.2f}% on {best_test[3] :.0f} epoch\")\n",
    "    print(f\"Best Test Previous: {best_test_previous[0]*100 :.2f}% on {best_test_previous[3] :.0f} epoch\")\n",
    "    \n",
    "    return parametr_set, counter, (previous_epochs+epochs), apr, anr, app, anp, conv3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image transformation (see dataset)\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of 10 capital letters\n",
    "24000 train images + 4000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of 10 capital letters from EMNIST\n",
    "\n",
    "path = f'./data/EMNIST_own/capital_letters/'\n",
    "\n",
    "test_letter_labels = torch.load(f'{path}Mozafari_capital_letters_test_labels.pt', map_location=torch.device('cpu'))\n",
    "test_letters = torch.load(f'{path}Mozafari_capital_letters_test_images.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "train_letter_labels = torch.load(f'{path}Mozafari_capital_letters_train_labels.pt', map_location=torch.device('cpu'))\n",
    "train_letters = torch.load(f'{path}Mozafari_capital_letters_train_images.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element permutation\n",
    "\n",
    "train_order_l = torch.randperm(train_letter_labels.shape[0])\n",
    "test_order_l = torch.randperm(test_letter_labels.shape[0])\n",
    "\n",
    "train_letter_labels = train_letter_labels[train_order_l].view(train_letter_labels.size())\n",
    "train_letters = train_letters[train_order_l].view(train_letters.size())\n",
    "\n",
    "test_letter_labels = test_letter_labels[test_order_l].view(test_letter_labels.size())\n",
    "test_letters = test_letters[test_order_l].view(test_letters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24000]), torch.Size([4000]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_letter_labels.size(), test_letter_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_letter_set = CustomTensorDataset(tensors=(train_letters, train_letter_labels), transform=s1c1_PIL)\n",
    "test_letter_set = CustomTensorDataset(tensors=(test_letters, test_letter_labels), transform=s1c1_PIL)\n",
    "\n",
    "train_letter_loader = DataLoader(train_letter_set, batch_size=len(train_letter_set))\n",
    "test_letter_loader = DataLoader(test_letter_set, batch_size=len(test_letter_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of 10 MNIST digits\n",
    "Reduction the set of 60000 train + 10000 test images to the set of 24000 train + 4000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the set of 10 digit images, the same size as the set of letters (2400 trains + 400 tests per class)\n",
    "\n",
    "# the MNIST data was pre-divided into 10 classes\n",
    "path = f'./data/MNIST_0_1_2_3_4_5_6_7_8_9/'\n",
    "\n",
    "for i in classes: \n",
    "    globals()[f'train_digit_{i}_images'] = torch.load(f'{path}train_images_{i}.pt', map_location=torch.device('cpu'))[0:2400]\n",
    "    globals()[f'train_digit_{i}_labels'] = torch.load(f'{path}train_labels_{i}.pt', map_location=torch.device('cpu'))[0:2400]\n",
    "    globals()[f'test_digit_{i}_images'] = torch.load(f'{path}test_images_{i}.pt', map_location=torch.device('cpu'))[0:400]\n",
    "    globals()[f'test_digit_{i}_labels'] = torch.load(f'{path}test_labels_{i}.pt', map_location=torch.device('cpu'))[0:400]\n",
    "\n",
    "train_MNIST_labels = globals()[f'train_digit_0_labels']\n",
    "train_MNIST_images = globals()[f'train_digit_0_images']\n",
    "test_MNIST_labels = globals()[f'test_digit_0_labels']\n",
    "test_MNIST_images = globals()[f'test_digit_0_images']                                 \n",
    "\n",
    "for i in range(1, 10):\n",
    "    train_MNIST_labels = torch.cat((train_MNIST_labels, globals()[f'train_digit_{i}_labels']), 0)\n",
    "    train_MNIST_images = torch.cat((train_MNIST_images, globals()[f'train_digit_{i}_images']), 0)\n",
    "\n",
    "    test_MNIST_labels = torch.cat((test_MNIST_labels, globals()[f'test_digit_{i}_labels']), 0)\n",
    "    test_MNIST_images = torch.cat((test_MNIST_images, globals()[f'test_digit_{i}_images']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24000]), torch.Size([4000]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MNIST_labels.size(), test_MNIST_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element permutation\n",
    "\n",
    "train_order = torch.randperm(train_MNIST_labels.shape[0])\n",
    "test_order = torch.randperm(test_MNIST_labels.shape[0])\n",
    "\n",
    "train_MNIST_labels = train_MNIST_labels[train_order].view(train_MNIST_labels.size())\n",
    "train_MNIST_images = train_MNIST_images[train_order].view(train_MNIST_images.size())\n",
    "\n",
    "test_MNIST_labels = test_MNIST_labels[test_order].view(test_MNIST_labels.size())\n",
    "test_MNIST_images = test_MNIST_images[test_order].view(test_MNIST_images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MNIST_set = CustomTensorDataset(tensors=(train_MNIST_images, train_MNIST_labels), transform=s1c1_PIL)\n",
    "test_MNIST_set = CustomTensorDataset(tensors=(test_MNIST_images, test_MNIST_labels), transform=s1c1_PIL)\n",
    "\n",
    "train_MNIST_loader = DataLoader(train_MNIST_set, batch_size=len(train_MNIST_set))\n",
    "test_MNIST_loader = DataLoader(test_MNIST_set, batch_size=len(test_MNIST_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mozafari = MozafariMNIST2018()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MozafariMNIST2018(\n",
       "  (conv1): Convolution()\n",
       "  (conv2): Convolution()\n",
       "  (conv3): Convolution()\n",
       "  (stdp1): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       "  (stdp2): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       "  (stdp3): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       "  (anti_stdp3): STDP(\n",
       "    (conv_layer): Convolution()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    mozafari.cuda()   \n",
    "mozafari.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous learning \n",
    "Training on set of letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving parameters before training on digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_from_scratch = {'stdp1': [mozafari.stdp1.learning_rate[0][0].item(), \n",
    "                                        mozafari.stdp1.learning_rate[0][1].item()],\n",
    "                              'stdp2': [mozafari.stdp2.learning_rate[0][0].item(), \n",
    "                                        mozafari.stdp2.learning_rate[0][1].item()],\n",
    "                              'stdp3': [mozafari.stdp3.learning_rate[0][0].item(), \n",
    "                                        mozafari.stdp3.learning_rate[0][1].item()],\n",
    "                              'anti_stdp3': [mozafari.anti_stdp3.learning_rate[0][0].item(), \n",
    "                                             mozafari.anti_stdp3.learning_rate[0][1].item()]\n",
    "                             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of SNN trained on 24,000 images of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file \"saved_24000_digits.net\" is the result of the file \"Initial_learning_of_SNN_on_digits.ipynb\"\n",
    "mozafari.load_state_dict(torch.load(\"saved_24000_digits.net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the first layer\n",
      "Epoch 0\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 1\n",
      "Iteration 0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the first layer\")\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(\"Epoch\", epoch)\n",
    "    iter = 0\n",
    "    for data, targets in train_letter_loader:\n",
    "        print(\"Iteration\", iter)\n",
    "        train_unsupervise(mozafari, data, 1)\n",
    "        print(\"Done!\")\n",
    "        iter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the second layer\n",
      "Epoch 0\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 1\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 2\n",
      "Iteration 0\n",
      "Done!\n",
      "Epoch 3\n",
      "Iteration 0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the second layer\")\n",
    "\n",
    "for epoch in range(4):\n",
    "    print(\"Epoch\", epoch)\n",
    "    iter = 0\n",
    "    for data,targets in train_letter_loader:\n",
    "        print(\"Iteration\", iter)\n",
    "        train_unsupervise(mozafari, data, 2)\n",
    "        print(\"Done!\")\n",
    "        iter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the third layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving learning_rates \n",
    "\n",
    "for i in range(len(mozafari.stdp3.learning_rate)):\n",
    "                    mozafari.stdp3.learning_rate[i][0].fill_(learning_rate_from_scratch['stdp3'][0])\n",
    "                    mozafari.stdp3.learning_rate[i][1].fill_(learning_rate_from_scratch['stdp3'][1])\n",
    "                    mozafari.anti_stdp3.learning_rate[i][0].fill_(learning_rate_from_scratch['anti_stdp3'][0])\n",
    "                    mozafari.anti_stdp3.learning_rate[i][1].fill_(learning_rate_from_scratch['anti_stdp3'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial adaptive learning rates\n",
    "\n",
    "apr = mozafari.stdp3.learning_rate[0][0].item()\n",
    "anr = mozafari.stdp3.learning_rate[0][1].item()\n",
    "app = mozafari.anti_stdp3.learning_rate[0][1].item()\n",
    "anp = mozafari.anti_stdp3.learning_rate[0][0].item()\n",
    "               \n",
    "parametr_set = pd.DataFrame(columns=['epoch', 'train', 'test', 'test_previous',   \n",
    "                                 'apr_adapt', 'anr_adapt', 'app_adapt', 'anp_adapt', \n",
    "                                 'stdp3.learning_rate[0]', 'stdp3.learning_rate[1]',\n",
    "                                 'anti_stdp3.learning_rate[0]', 'anti_stdp3.learning_rate[1]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will be performed without weight moving.\n",
      "--------------------------------------------------\n",
      "Epoch #:  0\n",
      "Current Train: 53.41%\n",
      "Current Test: 62.70%\n",
      "Current Test Previous: 77.28%\n",
      "Operational time of epoch #0: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  1\n",
      "Current Train: 63.86%\n",
      "Current Test: 65.03%\n",
      "Current Test Previous: 76.30%\n",
      "Operational time of epoch #1: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  2\n",
      "Current Train: 67.67%\n",
      "Current Test: 67.92%\n",
      "Current Test Previous: 79.40%\n",
      "Operational time of epoch #2: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  3\n",
      "Current Train: 69.89%\n",
      "Current Test: 69.75%\n",
      "Current Test Previous: 79.80%\n",
      "Operational time of epoch #3: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  4\n",
      "Current Train: 71.89%\n",
      "Current Test: 70.97%\n",
      "Current Test Previous: 80.45%\n",
      "Operational time of epoch #4: 2 min 6 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  5\n",
      "Current Train: 73.08%\n",
      "Current Test: 71.88%\n",
      "Current Test Previous: 80.88%\n",
      "Operational time of epoch #5: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  6\n",
      "Current Train: 73.95%\n",
      "Current Test: 72.38%\n",
      "Current Test Previous: 81.10%\n",
      "Operational time of epoch #6: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  7\n",
      "Current Train: 74.65%\n",
      "Current Test: 73.30%\n",
      "Current Test Previous: 81.08%\n",
      "Operational time of epoch #7: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  8\n",
      "Current Train: 75.29%\n",
      "Current Test: 74.02%\n",
      "Current Test Previous: 80.73%\n",
      "Operational time of epoch #8: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  9\n",
      "Current Train: 75.94%\n",
      "Current Test: 75.02%\n",
      "Current Test Previous: 80.47%\n",
      "Operational time of epoch #9: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  10\n",
      "Current Train: 76.45%\n",
      "Current Test: 75.72%\n",
      "Current Test Previous: 80.25%\n",
      "Operational time of epoch #10: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  11\n",
      "Current Train: 76.90%\n",
      "Current Test: 76.30%\n",
      "Current Test Previous: 79.95%\n",
      "Operational time of epoch #11: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  12\n",
      "Current Train: 77.47%\n",
      "Current Test: 76.78%\n",
      "Current Test Previous: 79.60%\n",
      "Operational time of epoch #12: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  13\n",
      "Current Train: 78.03%\n",
      "Current Test: 77.00%\n",
      "Current Test Previous: 79.38%\n",
      "Operational time of epoch #13: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  14\n",
      "Current Train: 78.35%\n",
      "Current Test: 77.48%\n",
      "Current Test Previous: 79.17%\n",
      "Operational time of epoch #14: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  15\n",
      "Current Train: 78.99%\n",
      "Current Test: 77.90%\n",
      "Current Test Previous: 78.65%\n",
      "Operational time of epoch #15: 2 min 6 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  16\n",
      "Current Train: 79.46%\n",
      "Current Test: 78.15%\n",
      "Current Test Previous: 78.35%\n",
      "Operational time of epoch #16: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  17\n",
      "Current Train: 79.77%\n",
      "Current Test: 78.17%\n",
      "Current Test Previous: 78.67%\n",
      "Operational time of epoch #17: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  18\n",
      "Current Train: 80.04%\n",
      "Current Test: 78.62%\n",
      "Current Test Previous: 78.85%\n",
      "Operational time of epoch #18: 2 min 11 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  19\n",
      "Current Train: 80.30%\n",
      "Current Test: 78.88%\n",
      "Current Test Previous: 78.83%\n",
      "Operational time of epoch #19: 2 min 7 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  20\n",
      "Current Train: 80.67%\n",
      "Current Test: 79.38%\n",
      "Current Test Previous: 78.53%\n",
      "Operational time of epoch #20: 2 min 8 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  21\n",
      "Current Train: 81.02%\n",
      "Current Test: 79.35%\n",
      "Current Test Previous: 78.85%\n",
      "Operational time of epoch #21: 2 min 8 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  22\n",
      "Current Train: 81.34%\n",
      "Current Test: 79.67%\n",
      "Current Test Previous: 78.65%\n",
      "Operational time of epoch #22: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  23\n",
      "Current Train: 81.62%\n",
      "Current Test: 79.90%\n",
      "Current Test Previous: 78.67%\n",
      "Operational time of epoch #23: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  24\n",
      "Current Train: 81.92%\n",
      "Current Test: 80.17%\n",
      "Current Test Previous: 78.62%\n",
      "Operational time of epoch #24: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  25\n",
      "Current Train: 82.10%\n",
      "Current Test: 80.62%\n",
      "Current Test Previous: 78.62%\n",
      "Operational time of epoch #25: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  26\n",
      "Current Train: 82.50%\n",
      "Current Test: 81.17%\n",
      "Current Test Previous: 78.35%\n",
      "Operational time of epoch #26: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  27\n",
      "Current Train: 82.78%\n",
      "Current Test: 81.35%\n",
      "Current Test Previous: 78.45%\n",
      "Operational time of epoch #27: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  28\n",
      "Current Train: 83.13%\n",
      "Current Test: 81.65%\n",
      "Current Test Previous: 78.38%\n",
      "Operational time of epoch #28: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  29\n",
      "Current Train: 83.29%\n",
      "Current Test: 81.85%\n",
      "Current Test Previous: 78.17%\n",
      "Operational time of epoch #29: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  30\n",
      "Current Train: 83.58%\n",
      "Current Test: 82.08%\n",
      "Current Test Previous: 77.98%\n",
      "Operational time of epoch #30: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  31\n",
      "Current Train: 83.79%\n",
      "Current Test: 82.47%\n",
      "Current Test Previous: 78.00%\n",
      "Operational time of epoch #31: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  32\n",
      "Current Train: 84.13%\n",
      "Current Test: 82.55%\n",
      "Current Test Previous: 77.90%\n",
      "Operational time of epoch #32: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  33\n",
      "Current Train: 84.34%\n",
      "Current Test: 82.58%\n",
      "Current Test Previous: 77.95%\n",
      "Operational time of epoch #33: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  34\n",
      "Current Train: 84.43%\n",
      "Current Test: 82.83%\n",
      "Current Test Previous: 77.90%\n",
      "Operational time of epoch #34: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  35\n",
      "Current Train: 84.80%\n",
      "Current Test: 83.00%\n",
      "Current Test Previous: 77.85%\n",
      "Operational time of epoch #35: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  36\n",
      "Current Train: 84.97%\n",
      "Current Test: 82.80%\n",
      "Current Test Previous: 77.75%\n",
      "Operational time of epoch #36: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  37\n",
      "Current Train: 85.06%\n",
      "Current Test: 83.20%\n",
      "Current Test Previous: 78.10%\n",
      "Operational time of epoch #37: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  38\n",
      "Current Train: 85.30%\n",
      "Current Test: 83.28%\n",
      "Current Test Previous: 77.50%\n",
      "Operational time of epoch #38: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  39\n",
      "Current Train: 85.48%\n",
      "Current Test: 83.47%\n",
      "Current Test Previous: 77.53%\n",
      "Operational time of epoch #39: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  40\n",
      "Current Train: 85.60%\n",
      "Current Test: 83.55%\n",
      "Current Test Previous: 77.85%\n",
      "Operational time of epoch #40: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  41\n",
      "Current Train: 85.79%\n",
      "Current Test: 83.85%\n",
      "Current Test Previous: 77.55%\n",
      "Operational time of epoch #41: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  42\n",
      "Current Train: 85.90%\n",
      "Current Test: 83.95%\n",
      "Current Test Previous: 77.60%\n",
      "Operational time of epoch #42: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  43\n",
      "Current Train: 86.11%\n",
      "Current Test: 84.20%\n",
      "Current Test Previous: 77.35%\n",
      "Operational time of epoch #43: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  44\n",
      "Current Train: 86.22%\n",
      "Current Test: 84.35%\n",
      "Current Test Previous: 77.65%\n",
      "Operational time of epoch #44: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Train: 86.39%\n",
      "Current Test: 84.45%\n",
      "Current Test Previous: 77.68%\n",
      "Operational time of epoch #45: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  46\n",
      "Current Train: 86.53%\n",
      "Current Test: 84.58%\n",
      "Current Test Previous: 77.72%\n",
      "Operational time of epoch #46: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  47\n",
      "Current Train: 86.77%\n",
      "Current Test: 84.60%\n",
      "Current Test Previous: 77.45%\n",
      "Operational time of epoch #47: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  48\n",
      "Current Train: 86.86%\n",
      "Current Test: 84.75%\n",
      "Current Test Previous: 77.25%\n",
      "Operational time of epoch #48: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  49\n",
      "Current Train: 86.97%\n",
      "Current Test: 85.08%\n",
      "Current Test Previous: 77.15%\n",
      "Operational time of epoch #49: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  50\n",
      "Current Train: 87.19%\n",
      "Current Test: 85.30%\n",
      "Current Test Previous: 77.10%\n",
      "Operational time of epoch #50: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  51\n",
      "Current Train: 87.21%\n",
      "Current Test: 85.35%\n",
      "Current Test Previous: 77.25%\n",
      "Operational time of epoch #51: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  52\n",
      "Current Train: 87.42%\n",
      "Current Test: 85.47%\n",
      "Current Test Previous: 77.20%\n",
      "Operational time of epoch #52: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  53\n",
      "Current Train: 87.62%\n",
      "Current Test: 85.38%\n",
      "Current Test Previous: 76.88%\n",
      "Operational time of epoch #53: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  54\n",
      "Current Train: 87.59%\n",
      "Current Test: 85.52%\n",
      "Current Test Previous: 76.98%\n",
      "Operational time of epoch #54: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  55\n",
      "Current Train: 87.81%\n",
      "Current Test: 85.70%\n",
      "Current Test Previous: 76.53%\n",
      "Operational time of epoch #55: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  56\n",
      "Current Train: 87.84%\n",
      "Current Test: 85.82%\n",
      "Current Test Previous: 76.58%\n",
      "Operational time of epoch #56: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  57\n",
      "Current Train: 87.99%\n",
      "Current Test: 85.82%\n",
      "Current Test Previous: 76.38%\n",
      "Operational time of epoch #57: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  58\n",
      "Current Train: 88.09%\n",
      "Current Test: 85.82%\n",
      "Current Test Previous: 76.42%\n",
      "Operational time of epoch #58: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  59\n",
      "Current Train: 88.10%\n",
      "Current Test: 85.90%\n",
      "Current Test Previous: 76.15%\n",
      "Operational time of epoch #59: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  60\n",
      "Current Train: 88.21%\n",
      "Current Test: 85.90%\n",
      "Current Test Previous: 76.08%\n",
      "Operational time of epoch #60: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  61\n",
      "Current Train: 88.22%\n",
      "Current Test: 86.00%\n",
      "Current Test Previous: 75.88%\n",
      "Operational time of epoch #61: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  62\n",
      "Current Train: 88.27%\n",
      "Current Test: 86.00%\n",
      "Current Test Previous: 75.95%\n",
      "Operational time of epoch #62: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  63\n",
      "Current Train: 88.27%\n",
      "Current Test: 85.95%\n",
      "Current Test Previous: 75.62%\n",
      "Operational time of epoch #63: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  64\n",
      "Current Train: 88.41%\n",
      "Current Test: 86.10%\n",
      "Current Test Previous: 75.45%\n",
      "Operational time of epoch #64: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  65\n",
      "Current Train: 88.52%\n",
      "Current Test: 86.15%\n",
      "Current Test Previous: 75.72%\n",
      "Operational time of epoch #65: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  66\n",
      "Current Train: 88.56%\n",
      "Current Test: 86.22%\n",
      "Current Test Previous: 75.28%\n",
      "Operational time of epoch #66: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  67\n",
      "Current Train: 88.59%\n",
      "Current Test: 86.28%\n",
      "Current Test Previous: 75.05%\n",
      "Operational time of epoch #67: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  68\n",
      "Current Train: 88.68%\n",
      "Current Test: 86.20%\n",
      "Current Test Previous: 75.17%\n",
      "Operational time of epoch #68: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  69\n",
      "Current Train: 88.73%\n",
      "Current Test: 86.52%\n",
      "Current Test Previous: 74.98%\n",
      "Operational time of epoch #69: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  70\n",
      "Current Train: 88.87%\n",
      "Current Test: 86.30%\n",
      "Current Test Previous: 75.17%\n",
      "Operational time of epoch #70: 2 min 1 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  71\n",
      "Current Train: 88.85%\n",
      "Current Test: 86.62%\n",
      "Current Test Previous: 74.48%\n",
      "Operational time of epoch #71: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  72\n",
      "Current Train: 88.95%\n",
      "Current Test: 86.70%\n",
      "Current Test Previous: 74.78%\n",
      "Operational time of epoch #72: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  73\n",
      "Current Train: 89.01%\n",
      "Current Test: 86.95%\n",
      "Current Test Previous: 74.60%\n",
      "Operational time of epoch #73: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  74\n",
      "Current Train: 89.04%\n",
      "Current Test: 86.72%\n",
      "Current Test Previous: 74.75%\n",
      "Operational time of epoch #74: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  75\n",
      "Current Train: 89.18%\n",
      "Current Test: 87.00%\n",
      "Current Test Previous: 74.38%\n",
      "Operational time of epoch #75: 2 min 7 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  76\n",
      "Current Train: 89.20%\n",
      "Current Test: 87.17%\n",
      "Current Test Previous: 74.08%\n",
      "Operational time of epoch #76: 2 min 7 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  77\n",
      "Current Train: 89.32%\n",
      "Current Test: 87.10%\n",
      "Current Test Previous: 74.17%\n",
      "Operational time of epoch #77: 2 min 6 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  78\n",
      "Current Train: 89.39%\n",
      "Current Test: 87.15%\n",
      "Current Test Previous: 74.20%\n",
      "Operational time of epoch #78: 2 min 9 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  79\n",
      "Current Train: 89.43%\n",
      "Current Test: 87.25%\n",
      "Current Test Previous: 74.48%\n",
      "Operational time of epoch #79: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  80\n",
      "Current Train: 89.49%\n",
      "Current Test: 87.20%\n",
      "Current Test Previous: 73.72%\n",
      "Operational time of epoch #80: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  81\n",
      "Current Train: 89.49%\n",
      "Current Test: 87.33%\n",
      "Current Test Previous: 73.72%\n",
      "Operational time of epoch #81: 2 min 6 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  82\n",
      "Current Train: 89.58%\n",
      "Current Test: 87.25%\n",
      "Current Test Previous: 73.47%\n",
      "Operational time of epoch #82: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  83\n",
      "Current Train: 89.50%\n",
      "Current Test: 87.10%\n",
      "Current Test Previous: 73.78%\n",
      "Operational time of epoch #83: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  84\n",
      "Current Train: 89.63%\n",
      "Current Test: 87.42%\n",
      "Current Test Previous: 73.67%\n",
      "Operational time of epoch #84: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  85\n",
      "Current Train: 89.78%\n",
      "Current Test: 87.45%\n",
      "Current Test Previous: 73.65%\n",
      "Operational time of epoch #85: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  86\n",
      "Current Train: 89.77%\n",
      "Current Test: 87.45%\n",
      "Current Test Previous: 73.75%\n",
      "Operational time of epoch #86: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  87\n",
      "Current Train: 89.80%\n",
      "Current Test: 87.20%\n",
      "Current Test Previous: 73.58%\n",
      "Operational time of epoch #87: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  88\n",
      "Current Train: 89.80%\n",
      "Current Test: 87.42%\n",
      "Current Test Previous: 73.72%\n",
      "Operational time of epoch #88: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  89\n",
      "Current Train: 89.97%\n",
      "Current Test: 87.35%\n",
      "Current Test Previous: 73.88%\n",
      "Operational time of epoch #89: 2 min 4 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  90\n",
      "Current Train: 90.05%\n",
      "Current Test: 87.45%\n",
      "Current Test Previous: 73.98%\n",
      "Operational time of epoch #90: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Train: 90.08%\n",
      "Current Test: 87.35%\n",
      "Current Test Previous: 73.58%\n",
      "Operational time of epoch #91: 2 min 5 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  92\n",
      "Current Train: 90.00%\n",
      "Current Test: 87.38%\n",
      "Current Test Previous: 74.12%\n",
      "Operational time of epoch #92: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  93\n",
      "Current Train: 90.12%\n",
      "Current Test: 87.45%\n",
      "Current Test Previous: 73.60%\n",
      "Operational time of epoch #93: 2 min 3 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  94\n",
      "Current Train: 90.12%\n",
      "Current Test: 87.48%\n",
      "Current Test Previous: 74.12%\n",
      "Operational time of epoch #94: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  95\n",
      "Current Train: 90.15%\n",
      "Current Test: 87.50%\n",
      "Current Test Previous: 74.00%\n",
      "Operational time of epoch #95: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  96\n",
      "Current Train: 90.18%\n",
      "Current Test: 87.35%\n",
      "Current Test Previous: 73.83%\n",
      "Operational time of epoch #96: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  97\n",
      "Current Train: 90.13%\n",
      "Current Test: 87.45%\n",
      "Current Test Previous: 73.92%\n",
      "Operational time of epoch #97: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  98\n",
      "Current Train: 90.10%\n",
      "Current Test: 87.60%\n",
      "Current Test Previous: 74.38%\n",
      "Operational time of epoch #98: 2 min 2 sec\n",
      "--------------------------------------------------\n",
      "Epoch #:  99\n",
      "Current Train: 90.19%\n",
      "Current Test: 87.65%\n",
      "Current Test Previous: 73.50%\n",
      "Operational time of epoch #99: 2 min 3 sec\n",
      "==================== SUMMARY ====================\n",
      "        Best Train: 90.19% on 99 epoch\n",
      "         Best Test: 87.65% on 99 epoch\n",
      "Best Test Previous: 81.10% on 6 epoch\n"
     ]
    }
   ],
   "source": [
    "# train the 3-rd layer\n",
    "\n",
    "first_test = third_layer(file_name_net=\"saved_letters_after_digits_total_0.net\",\n",
    "                        file_name_csv='parameter_set_letters_after_digits_0.csv',\n",
    "                        adaptive_int=0.5, previous_epochs=0, epochs=100, \n",
    "                        train_loader=train_letter_loader, \n",
    "                        test_loader=test_letter_loader, \n",
    "                        test_previous_loader=test_MNIST_loader,\n",
    "                        model=mozafari, apr=apr, anr=anr, app=app, anp=anp, \n",
    "                        parametr_set=parametr_set, \n",
    "                        steps=0, percent=0)\n",
    "\n",
    "parametr_set = first_test[0] \n",
    "counter = first_test[1] \n",
    "previous_epochs = first_test[2]\n",
    "apr = first_test[3] \n",
    "anr = first_test[4] \n",
    "app = first_test[5] \n",
    "anp = first_test[6]\n",
    "conv3_data_train = first_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "curve_graph(parametr_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
